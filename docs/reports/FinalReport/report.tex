\documentclass[12pt]{article}

\usepackage{scicite,times,graphicx,float,hyperref}
\usepackage[skip=0pt]{caption}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{siunitx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PREAMBLE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\topmargin -1.0cm
\oddsidemargin 0.0cm 
\textwidth 16cm 
\textheight 23cm
\footskip 1.0cm

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}

\newcounter{lastnote}
\newenvironment{scilastnote}{%
  \setcounter{lastnote}{\value{enumiv}}%
  \addtocounter{lastnote}{+1}%
  \begin{list}%
  {\arabic{lastnote}.}
  {\setlength{\leftmargin}{.22in}}
  {\setlength{\labelsep}{.5em}}
}
{\end{list}}

\sisetup{
  round-mode          = places, % Rounds numbers
  round-precision     = 2, % to 2 places
}

\title{Final Report on:\\Pretix Redundant Operation} 

\author
{Filipe Pires [85122], Jo√£o Alegria [85048]\\
\\
Computational Infrastructures Management\\
\normalsize{Department of Electronics, Telecommunications and Informatics}\\
\normalsize{University of Aveiro}\\
} 
 
\date{\today{}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% REPORT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\baselineskip18pt

\maketitle

\section*{Introduction} \label{introduction} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This report aims to describe the work developed for the final phase of the practical assignment of the discipline of Computational Infrastructures
Management \cite{assign} from the Msc. degree in Informatics Engineering of the University of Aveiro at the Department of Electronics, Telecommunications and
Informatics.
It is assumed that the reader has knowledge about the previous report, and it is here included:
the characterization of the service level agreements for each service;
the description of the load balancing mechanisms among software components;
the redundancy strategy;
the listing of metrics and computational resources monitored, along with the respective defined alarms;
and other relevant aspects such as horizontal scalability and component fault tolerance.

The service provided is Pretix, an online shop, box office and ticket outlet already successfully used by other service providers for conferences, festivals,
exhibitions, workshops and more.
The previously delivered work focused on the product presentation, distributed installation, and resource and performance analysis.
All code developed is publicly accessible in our GitHub repository:

\url{https://github.com/FilipePires98/GIC}.

% \cite{pretix} \cite{rami.io} 
% \cite{pretixdoc} \cite{pretixgit}
% \cite{docker} \cite{django} \cite{gunicorn} \cite{nginx}
% \cite{pretix_img}
% \cite{postgresql} \cite{redis} \cite{pgpool} \cite{haproxy}
% \cite{ansible}

% \texttt{java -cp <userdir>/build/classes fi.FarmInfrastructure}

% \vspace{-10pt}
% \begin{itemize}[noitemsep]
%   \item ...
% \end{itemize}
% \vspace{-10pt}

% \vspace{-10pt}
% \begin{enumerate}[noitemsep]
%   \item ...
% \end{enumerate}
% \vspace{-10pt}

% \begin{figure}[H]
%   \centering
%   \begin{minipage}{\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{img/.....png}
%   \end{minipage}%
%   \caption{...}
%   \label{...}
% \end{figure} 

% \begin{figure}[H]
%   \centering
%   \begin{minipage}{\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{tests/local/800/number_of_users_1588097887.png}
%   \end{minipage}%
%   \label{fig:LocalTests:number_of_users}
%   \begin{minipage}{\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{tests/local/800/response_times_(ms)_1588097887.png}
%   \end{minipage}%
%   \label{fig:LocalTests:response_times_(ms)}
%   \begin{minipage}{\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{tests/local/800/total_requests_per_second_1588097887.png}
%   \end{minipage}%
%   \caption{Load tests results with 800 users.}
%   \label{fig:LocalTests:total_requests_per_second}
% \end{figure}
% \vspace{-10pt}

\newpage
\section{System Architecture} \label{architecture} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Infrastructure} \label{architecture.infrastructure} %%%%%%%%%%%%% 

% What is the architecture of the infrastructure deployed?

To provide the Pretix Ticketing Software \cite{pretix} to the public, it was required to set up an infrastructure containing their web application, an instance
of a Web Server Gateway Interface (WSGI) hosting the application and of a reverse-proxy for the web application deployment in production mode, as well as a
database management system (DBMS) (where PostgreSQL was used) for handling disk storage and a caching server (where Redis was chosen) for in-memory storage and
asynchronous queuing of tasks.

Our strategy is thoroughly described in the previous report, which already considered minimum redundancy to ensure availability.
It is achieved through the usage of Docker \cite{docker} to isolate each component for greater control and easier configuration, and Docker Swarm \cite{dockerswarm}
for the orchestration of the entire infrastructure.
Nevertheless, as it was a still primitive solution, naturally several upgrades were applied, mostly internally to each container.
The latest architecture is visually presented in Figure \ref{fig:InfrastructureArchitecture}.
This solution considers quality attributes whose assurance is described in the sections below.

\begin{figure}[H]
  \centering
  \begin{minipage}{.85\textwidth}
    \centering
    \includegraphics[width=\linewidth]{diagrams/InfrastructureArchitecture.png}
  \end{minipage}%
  \caption{Infrastructure architecture diagram deployed in Docker Swarm.}
  \label{fig:InfrastructureArchitecture}
\end{figure}

\subsection{Load Balancing} \label{architecture.loadbalancing} %%%%%%%%%%%%%

% How is load balancing assured?

The knowledge gathered so far regarding the operation of Pretix affirms that user activity peaks are to be expected when in production.
This has many implications and requires the implementation of mechanisms that maximize the infrastructure's efficiency.

With this in mind, we have come up with mechanisms for load balancing based on proxy servers.
These servers, when strategically placed, act as intermediaries for requests seeking resources from the system itself.
This not only reduces the complexity of requests made to our services, but also provides additional benefits with regards to security and load balancing,
as they allow a controlled and intelligent distribution of requests among components.

An NGinX proxy \cite{nginx} was installed between the end users and the Django-based web application.
With the help of Docker Swarm, replicas of both the proxy and the web application were deployed.
This is transparent for the user and it works as if he/she is sending requests to one single access point.

The storage clusters also resort to proxies.
For the PostgreSQL server cluster \cite{postgresql}, we adopted Pgpool-II \cite{pgpool}.
As our database is replicated in master/slave mode, Pgpool is able to take advantage of the replication feature in order to reduce the load on each PostgreSQL server
by evenly distributing queries among available servers and consequently improving the overall throughput.
Pgpool provides other useful features such as connection pooling, where it maintains established connections to the servers and reuses them whenever a new
connection with the same properties comes in, and automated fail over, mentioned in greater detail in section \ref{management.automation}.
For the Redis server cluster \cite{redis}, the adopted solution was the well-known HAProxy \cite{haproxy}.
The instances installed spread requests evenly across the Redis cluster. 
Here, a master/slave mode was also used, meaning that the proxies interact with master instances that then delegate work among slaves.

Load tests executed during development proved that such additions had a significant impact on the performance of the system.
The usage of networks, seen in Figure \ref{fig:InfrastructureArchitecture}, where the proxies mentioned above are inserted help orchestrating and logically
grouping components.

\subsection{Redundancy} \label{architecture.redundancy} %%%%%%%%%%%%%%%%%%%%

% How is redundancy assured?

In order for us to provide Pretix, it is only required to maintain one instance of each component running.
However, this is highly unadvisable as it susceptible to failure from any point, i.e. if any of the system components fails, the entire system goes down.
This is referred to as a redundancy measure of N.

Ensuring actual redundancy of a system requires at least a measure of N+1, where each component can be replaced by another if required.
In most cases this is achievable through the replication feature of Docker Swarm.
Pretix's stack has K replicas of NGinX, of the Pretix web application, of Pgpool proxy and of HAProxy, where K can be defined on deployment with a value greater than 2.

Redundancy was also the reason why a master/slave mode was adopted on both clusters.
By instantiating 1 master and K slaves, it is possible to fulfill the requirements implicit to N+1 since if any slave fails the others can replace it and if the
master fails a slave is elected the new master.
Fortunately, for the PostgreSQL cluster this reelection process is done automatically, but for the Redis cluster the use of sentinels is required: sentinels
observe the behavior of masters and slaves, detect failures and reassign roles when needed; here we insist on providing N+1 redundancy by having multiple
instances of sentinels as well.

\subsection{Scalability} \label{architecture.scalability} %%%%%%%%%%%%%%%%%%%%%

% How is horizontal scalability assured?

Any project intended to be successful must consider the scalability of the product in the future.
Ours is no exception since, if the ticketing platform becomes popular, the infrastructure must be prepared to respond to greater loads of income traffic.

There were 2 means to ensure scalability: vertical or horizontal.
As we had access to limited computational resources and adding more physical RAM or CPUs was not an option, the only alternative left was to support horizontal scaling.

The idea here is to make possible the deployment of new replicas of any component and integrate them to the existing infrastructure without significant negative
impact on insertion and in a completely transparent way to the users.
This is exactly what we achieved.
Additionally, each component identifies itself to the monitoring server on initialization so that the data related to what is being managed is constantly up-to-date.

Scaling a portion of the stack can be done both manually or in an automated way.
By default we support manual scaling, but we found it was logical to implement automated mechanisms to scale up or down according to the online users and the
resources used.
This was achieved through the monitoring of meaningful metrics collected from each container instance, as we present in section \ref{management.automation}.

\newpage
\section{Infrastructure Management} \label{management} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Service Level Agreement} \label{management.sla} %%%%%%%%%%%%%%%

% What are the SLAs defined?

As service providers, we are supposed to fulfill a commitment with our clients, whether it is formally defined or not.
Obviously it is a good practice to elaborate an agreement where particular aspects of the service are agreed between the service provider and the service user -
this is called a Service Level Agreement (SLA).
We describe ours in this section.

Considering the scope of our project, we've defined the following commitment key points:
\vspace{-10pt}
\begin{itemize}[noitemsep]
  \item Pretix is to be provided through web browsers, by accessing a particular address, and it will only be accessible to those within the university's virtual private network (VPN).
  \item Its usage is fully dependent on the usability of the Pretix service itself and no complexity should be added by the infrastructure we provide with regards to this.
  \item We are responsible for providing free access to the ticketing platform through our servers and keeping data integrity even when the service is unavailable
  \item The installations hosting our deployment belong to a third party (the university) and thus we are not accountable for possible availability issues related to networking and machine up time.
        Nevertheless, if this is not considered, we ensure an availability of 100\% under normal circumstances and of 70\% for high activity peaks \footnote{Over 50 requests per minute.}, with a maximum response time of 50 seconds for ticket purchasing requests.
  \item No support is guaranteed for issues about features related to the internal implementation of the ticketing software.
\end{itemize}
\vspace{-10pt}

It is worth mentioning that our analysis to the (free version of the) Pretix product showed that the software had strong limitations on maximum load capacity
and response time that, according to the authors, could not be solved through horizontal scaling.

Focusing on the infrastructure itself and the computational resources, we have defined a list of Service Level Objects (SLO) - the metrics to be observed -
representative of the stack's state, and a list of Service Level Indications (SLI) - the thresholds or functions applied to the metric values - that will
help us react to situations where the SLA compliance is threatened.
These SLOs revolve around: hardware capacity, operation latency, availability percentage and reliability.

Table \ref{tab:sla} presents a simplified view on the metrics used to monitor the SLA and some thresholds that aid on the prevention of a cop-out.
Note that the chosen metrics are dependent on what is made available by each component.

\begin{table}[H]
  \begin{center}
    \begin{tabular}{l|l|r}
      \hline
      \textbf{Category} & \textbf{Metric}           & \textbf{Upper Threshold} \\
      \hline
      Hardware          & System Up Time            & -                        \\
      (SNMP)            & \% User CPU Time          & 90\%                     \\
                        & Total RAM                 & -                        \\
                        & Total RAM Used            & -                        \\
                        & \% RAM Usage              & 90\%                     \\
                        & Total Disk                & -                        \\
                        & Total Disk Used           & -                        \\
                        & \% Disk Usage             & 90\%                     \\
      \hline
      Pretix            & Total \# of Events        & -                        \\
                        & Total \# of Orders        & -                        \\
                        & Total \# of Purchases     & -                        \\
                        & Response Time             & 50 seconds               \\
      \hline
      NGinX             & Total \# of Requests      & 10\% (failure)           \\
                        & Avg. Response Time        & 50 seconds               \\
      \hline
      Redis             & Redis Up Time             & -                        \\
                        & Memory Used               & -                        \\
                        & \# of Connections         & -                        \\
                        & \# of Connected clients   & -                        \\
                        & Avg. Response Time        & -                        \\
      \hline
      PostgreSQL        & Total \# of Queries       & -                        \\
                        & Total \# of Failures      & -                        \\
                        & Max. Transaction Duration & 25 seconds               \\
      \hline
    \end{tabular}
    \caption{Monitored metrics, categorized by source.}
    \label{tab:sla}
  \end{center}
\end{table}
\vspace{-35pt}

\subsection{Monitoring} \label{management.monitoring} %%%%%%%%%%%%%%%%%%%%%

% How is the infrastructure monitored? 

Understanding the state of the infrastructure and components is essential for ensuring the correct functioning and stability of the service.
Information about the health and performance of deployments not only helps us react to issues, it also gives security to make changes with confidence.
One of the best ways to gain this insight is with a robust monitoring system that gathers logs and metrics, visually displays data, and alerts us operators when
undesired events occur or when the maximum capacity of some resource is almost reached.
This kind of system also allows the definition of reactions for specific SLA violation threats.

Our monitoring system is hosted on an independent machine with 32 cores of 2.1GHz, 64Gb of RAM and 7Tb of disk space.
Just as the infrastructure hosting our Pretix stack, this too was shared amongst other operator teams.
The virtual machine (VM) we instantiated for our monitoring system runs Ubuntu 20.04 LTS and has 2 CPU cores, 4Gb of RAM and 50Gb of disk space.
The separation between our product and its monitoring was understood to be optimal, since this way there is no added load to the service facility, keeping its
resources fully dedicated to itself.
There is also the advantage of allowing the deployment of our system both with or without monitoring enabled.

\subsubsection{Log Collection}

There are 3 major reasons for keeping log messages of software.
For developers, logging tools are often deployed in places where they don‚Äôt have access or there are no debugging tools, but log messages can still help to
localize problems.
For operators, or system administrators, they provide insight on the state of a group of connected machines, thus helping them to make sure all run as
smoothly as possible.
But perhaps the reason with greatest impact is security.
Log monitoring systems oversee network activity, inspect system events, and store user actions that occur inside the operating systems.
Reviewing this information is a strong means of detecting suspicious activity and identify the damage caused in case of an attack.

A centralized point of log monitoring simplifies this process of message analysis, so we installed a platform in our monitoring VM responsible for collecting
log files from each component, storing them and presenting the data in an organized, visually useful and empowering form.
There are a few options to achieve this and after some brainstorming and research, we found the Elasticsearch stack (ELK) \cite{elk} to be the most appropriate
solution for our scenario.
This stack comprises 3 open-source projects, each with its own purpose:

\vspace{-10pt}
\begin{itemize} [noitemsep]
  \item Elasticsearch - a search and analytics engine, optimized for textual data such as logs; logs are stored in it.
  \item Logstash - a data ingestion tool, that serves as the stack entrypoint; responsible for collecting, transforming and sending data to a desired destination.
  \item Kibana - a data visualization and exploration tool for reviewing logs and events; enables the creation of dashboards, alerts and more.
\end{itemize}
\vspace{-10pt}

The first approach attempted used rsyslog \cite{rsyslog}.
This technology allows the definition of a server and multiple clients, each one assigned to a service that gathers the system logs of the machine and sends
them to the predefined rsyslog server.
This would provide us with centralization, but several integration issues were faced.
Soon we found that it was not an optimal solution, since when receiving exterior logs it persists them in the system logs file of the hosting machine, resulting
in a duplication of logs both on hosting machines and on the monitoring VM's Elasticsearch (due to its indexing process).

The second approach ended up being much cleaner and simpler.
After some research we found that one of the logging drivers provided by the docker platform was syslog \cite{dockersyslog}.
Syslog enables the redirecting of all logs to a syslog server specified in the driver options - which was aligned with our scenario.
We also found that Logstash allows the creation of a server that can receive any type of message (since filters can be applied to the incoming messages),
enabling the direct connection between the docker syslog driver and the ELK stack.

After the logs arrive at Logstash, the internal ELK flow was configured so that Elasticsearch indexes the information and Kibana is able to access it.
The system is thus capable of collecting all logs and present them in the Kibana interface.
Further processing over collected logs can be made if the information is context specific and allows these type of processing, such as counters or other metrics.
However, in our case this was found unnecessary since we were able to obtain metrics from all the services necessary.

An example of the populated Kibana GUI is present in figure \ref{fig:kibana}.

\subsubsection{Metric Collection}

Metrics, monitoring, and alerting are all interrelated concepts with the ability to provide visibility into the health of a system, to help you understand trends
in usage or behavior, and to understand the impact of changes applied.
If the metrics fall outside of expected ranges, a monitoring system that incorporates such concepts can send notifications to prompt an operator to take a look,
can then assist in surfacing information to help identify the possible causes, and can even have predefined response measures triggered on specific states.

Centralizing metrics has the same benefits as doing so with log messages.
However, metrics do not provide a historical record of the system's behavior, rather they present in great detail the current state of the system, mostly through
numeric data like counters, gauges and service or context-specific indicators.

Naturally, there are a few approaches and technologies that allow metric collection.
After some research, we found some that are commonly used for this purpose, such as Nagios, Zabbix, Munin, TICK stack, Prometheus and more.
We resorted to Prometheus \cite{prometheus}, since our product Pretix - our main service - already exported metrics in a format compatible with Prometheus.

For the remaining services we installed parallel metric exporters compatible with Prometheus.
Since our infrastructure was deployed using docker in swarm mode, the addition of exporters was made by changing the Docker files and respective entry points
for each service.
The mapping between service and installed exporter is:

\vspace{-10pt}
\begin{itemize} [noitemsep]
  \item HA Proxy - \href{https://github.com/prometheus/haproxy_exporter}{Official Statistics Exporter}.
  \item Nginx - \href{https://github.com/hnlq715/nginx-vts-exporter}{VTS Statistics Exporter}.
  \item PostgreSQL Master/Slave - \href{https://github.com/wrouesnel/postgres_exporter}{Server Metrics Exporter}.
  \item Redis Master/Slave/Sentinel - \href{https://github.com/oliver006/redis_exporter}{Server Metrics Exporter}.
\end{itemize}
\vspace{-10pt}

An additional exporter was installed in the monitoring VM, so that hardware-related metrics could also be monitored, such as CPU, memory and disk usage,
To achieve this, we resorted to the Simple Network Management Protocol (SNMP), an internet standard protocol for collecting and organizing information about
managed devices on IP networks and for modifying that information to change device behavior.
The protocol support was installed \textit{a priori} on every compute node, being only necessary to request it's metrics when needed.
However, this posed an additional challenge: since the deployment is made in Swarm mode, it was impossible for us to know where our services would be deployed
in the Swarm computing network.
Statically indicating every computing node to be monitored would cause stress on machines and would not scale properly, so we implemented a dynamic IP address
discovery mechanism.

This custom address collector has an independent service and was created using Python's Flask library, which enabled us to quickly develop the necessary endpoints
and respective logic.
At initialization, each service sends an HTTP Post to our custom service with its name, replication number and identifier.
The collector service then registers or updates it's IP table and modifies the respective Prometheus target files for each type of exporter.

Our choice for the graphical user interface (GUI) to interact with Prometheus was Grafana \cite{grafana}, commonly integrated with Prometheus as it is the GUI
indicated by the engine itself.
Grafana allows us to explore the collected metrics, construct meaningful dashboards and configure critical alerts, as we will see further ahead.

Figures \ref{fig:home} to \ref{fig:snmp2} are all populated examples of the created dashboards, one for each type of service, namely: 
Nginx, PostgreSQL, Redis, Pretix and the SNMP infrastructure monitoring.

\subsection{Alerts and Automation} \label{management.automation} %%%%%%%%%%%%%%%

% SLA violation alarms
% How is the provisioning of new elements automated?
% How is the infrastructure fault tolerant? (pgpool automated failover, redis sentinels, SLA violation alarm responses)

At this stage we had built an arsenal for the operation of our product.
Guided by our SLA, we proceeded to defining meaningful alerts triggered by thresholds applied to specific metrics, and automatic reaction measures in order to
prevent agreement breaches provide flexibility to the whole infrastructure.
All the generated alarms and notifications are sent to a dedicated Microsoft Teams channel and to a custom service responsible for processing and reacting.
This service is none other that the IP address collector that, using Docker's Python SDK \cite{dockerpythonsdk}, is capable of automatically scaling up or down
a service that triggered a received alarm.
Following is a list of the implemented alarms and respective reaction behaviors:

\vspace{-10pt}
\begin{enumerate}[noitemsep]
  \item NGINX   - Pretix Response Time alert
  \item PRETIX  - Number of Events alert [for testing purposes]
  \item PRETIX  - Order POST Time Distribution alert          - On these 3 alerts a new replica of the \texttt{pretix\_web} is created, as they indicate an overload on the existing ones.
  \item REDIS   - \# Connected Clients alert
  \item HAPROXY - Proxy Average Response Time of Redis alert  - As Pretix might not be the one causing slower response times, these 2 alerts indicate the Redis cluster is close to its maximum capacity and create a new replica of the \texttt{pretix\_redisslave} to avoid it.
\end{enumerate}
\vspace{-10pt}

Alert number 2 was created for testing purposes only, this was our means to achieve a consistent and controlled manual trigger:
by defining a low threshold and simply creating Pretix hosted events, the alarm was triggered and the notification channels and the automatic rescaling reaction
mechanism could easily be tested.

\section{Additional Remarks} \label{remarks} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Swarm Stability} \label{remarks.stability} %%%%%%%%%%%%%%%%%%%%%

We have mentioned the fact that all remote computational resources were shared among developer/operator teams and naturally this had consequences in terms of 
performance and accessibility, with activity peaks close to delivery dates and occurrences of human errors with generalized consequences.
Additionally, due to external limitations, maintenance also suffered some difficulties and access to the machines was not 100\% assured.

To partially mitigate these limitations, much of our tests and debugging was executed with local deployments to allow an independent and frictionless evolution 
in each system's component.
Nevertheless, often we were forced to postpone necessary remote tests due to unexpected errors related to the instability of the connection to the remote 
hardware and lost time doing research on a problem who's solution wasn't at our reach.
These problems were predominant in the PostgreSQL cluster, since internally it's components need to establish a replication scheme by exchanging control messages
through the network; inconsistencies in these exchanges would then prevent the correct initialization of the cluster.
This was a reality we had to face during the project lifetime, but was fortunately reduced thanks to the efforts of those responsible for the assignment.

\subsection{Assignment Contributions} \label{remarks.contributions} %%%%%%%%%

% who did what?

Due to external issues that prevented physical proximity among students and professors, a confined approach had to be adopted during this second development phase.
Nevertheless, the close-contact strategy was kept as this project required a broad perspective on the infrastructure and the tasks to be implemented by both team members.
Regarding the work distribution among developers, tasks were assigned to each according to a predefined plan and a common working schedule was applied.

The technology-related decisions were first made in conjunction.
Then, Filipe set up the monitoring VM to gather the data from the stack while Jo√£o began to work on the mechanism to collect the logs from each container.
Filipe moved on to installing metric exporters on each container, and Jo√£o found means to collect lower level metrics using the SNMP.
The container identification mechanism of each instance to the monitoring system was then implemented.
At this stage, both operators worked on defining the SLA and Jo√£o designed the monitoring dashboards.
Finally, the alarms and automated response mechanisms were implemented.
It is needless to say that bug and error solving was made along the development phase by both developers any time it was required.
Once a stable deployment fulfilling all predefined requirements was completed, this report became our primary concern, with both contributing equally.

\section*{Conclusions} \label{conclusions} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

With the work conducted during these two development phases, we were able to achieve a robust solution for the remote automatic deployment, hosting and operation of Pretix.
Our delivered code provides a system who's architecture is fully redundant, tolerant to failures, scalable and with a level of automation of great value for the 
system administrators.
Moreover, it provides an organized and complete set of monitoring tools centralized in an independent access point.

It is our belief that now it is possible to continue the development of the service while ensuring the correct maintenance of what is deployed, i.e. not only 
is the infrastructure prepared to host online events, but it is also prepared to increment updates to the internal logic without compromising the overall stack 
of services.

For future work, the strategy would be to first conduct trials with real users in order to learn more about the system's behavior and determine what areas would 
benefit from additional automated response measures.
Although the offered monitoring dashboards already have great detail over the collected data, personalizing them to the specific case of Pretix would also help 
on providing end users with a better experience.
Lastly, the limitations of the Pretix service itself should also be addressed in order to improve the user load capacity for activity peaks and take full advantage 
of the scalability potential of our clusters.

The perspective of operators is perhaps the most valuable knowledge we take from this project.
Not only it empowered us with the capabilities to address similar projects in the future, it also made us more aware of aspects to take in consideration as 
software developers in order to improve and take full advantage of the DevOps environment.

\newpage
\begin{thebibliography}{9} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \bibliographystyle{Science}

  \bibitem{assign}
  J. P. Barraca,
  \textit{GIC - Report no.2: Redundant Product Operation},
  University of Aveiro,
  2019/20.
  \vspace{-10pt}

  \bibitem{pretix}
  \textit{About Pretix},
  \url{https://pretix.eu/about/en/}.
  Pretix.eu,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{docker}
  \textit{Docker Homepage},
  \url{https://www.docker.com/}.
  Docker Inc.,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{dockerswarm}
  \textit{Docker Docs: Getting Started with Swarm Mode},
  \url{https://docs.docker.com/engine/swarm/swarm-tutorial/}.
  Docker Inc.,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{nginx}
  \textit{NGinX News},
  \url{https://nginx.org/}.
  NGinX.org,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{postgresql}
  \textit{PostgreSQL: The World's Most Advanced Open Source Relational Database},
  \url{https://www.postgresql.org/}.
  The PostgreSQL Global Development Group,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{pgpool}
  \textit{Welcome to Pgpool-II},
  \url{https://www.pgpool.net/mediawiki/index.php/Main_Page}.
  PgPool Global Development Group,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{redis}
  \textit{About Redis},
  \url{https://redis.io/}.
  Redis Labs,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{haproxy}
  \textit{HAProxy: The Reliable, High Performance TCP/HTTP Load Balancer},
  \url{https://www.haproxy.org/}.
  HAProxy.org,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{elk}
  \textit{What is the ELK Stack},
  \url{https://www.elastic.co/what-is/elk-stack}.
  Apache Software Foundation,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{rsyslog}
  \textit{RSyslog: The Rocket-Fast Syslog Server},
  \url{https://www.rsyslog.com/}.
  Adiscon GmbH,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{dockersyslog}
  \textit{Docker Docs: Configure Logging Drivers},
  \url{https://docs.docker.com/config/containers/logging/configure/}.
  Docker Inc.,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{prometheus}
  \textit{Prometheus: from Metrics to Insight},
  \url{https://prometheus.io/}.
  The Linux Foundation,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{grafana}
  \textit{Grafana: the Analytics Platform for all your Metrics},
  \url{https://grafana.com/}.
  Grafana Labs,
  retrieved in June 2020.
  \vspace{-10pt}

  \bibitem{dockerpythonsdk}
  \textit{Docker SDK for Python},
  \url{https://docker-py.readthedocs.io/en/stable/}.
  Docker Inc.,
  retrieved in June 2020.
  \vspace{-10pt}

  % \bibitem{pretixgit}
  % \textit{Pretix Code Repository},
  % \url{https://github.com/pretix/pretix}.
  % GitHub, Inc.,
  % retrieved in June 2020.
  % \vspace{-10pt}

  % \bibitem{pretixdoc}
  % \textit{Welcome to pretix' documentation!},
  % \url{https://docs.pretix.eu/en/latest/}.
  % Pretix.eu,
  % retrieved in June 2020.
  % \vspace{-10pt}

  % \bibitem{pretix_img}
  % \textit{pretix/standalone},
  % \url{https://hub.docker.com/r/pretix/standalone}.
  % pretix,
  % retrieved in June 2020.
  % \vspace{-10pt}

  % \bibitem{gunicorn}
  % \textit{Gunicorn Homepage},
  % \url{https://gunicorn.org/}.
  % Gunicorn.org,
  % retrieved in June 2020.
  % \vspace{-10pt}

  % \bibitem{locust}
  % \textit{Locust, an open source load testing tool},
  % \url{https://locust.io/}.
  % Locust.io,
  % retrieved in June 2020.
  % \vspace{-10pt}

  % \bibitem{selenium}
  % \textit{About Selenium},
  % \url{https://www.selenium.dev/about/}.
  % Software Freedom Conservancy,
  % retrieved in June 2020.
  % \vspace{-10pt}

  % \bibitem{ansible}
  % \textit{Ansible},
  % \url{https://www.ansible.com/}.
  % Red Hat,
  % retrieved in June 2020.

\end{thebibliography}

\clearpage


\appendix
\section{Kibana and Grafana Populated GUI}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/kibana.png}
  \caption{Populated Kibana GUI.}
  \label{fig:kibana}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/home.png}
  \caption{Grafana Custom Home Dashboard.}
  \label{fig:home}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/pretixServiceDashboard.png}
  \caption{Populated Pretix Service Dashboard.}
  \label{fig:pretix}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/pretixRedisDashboard.png}
  \caption{Populated Redis Dashboard(Part1).}
  \label{fig:redis1}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/pretixRedis2Dashboard.png}
  \caption{Populated Redis Dashboard(Part2).}
  \label{fig:redis2}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/pretixPostgresDashboard.png}
  \caption{Populated PostgreSQL Dashboard.}
  \label{fig:postgres}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/pretixNginxDashboard.png}
  \caption{Populated Nginx Dashboard.}
  \label{fig:nginx}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/pretixSnmpDashboard.png}
  \caption{Populated SNMP Dashboard(Part1).}
  \label{fig:snmp1}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/pretixSnmp2Dashboard.png}
  \caption{Populated SNMP Dashboard(Part2).}
  \label{fig:snmp2}
\end{figure}

\end{document}